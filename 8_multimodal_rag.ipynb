{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01aa8778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishekkuber/Desktop/Upskilling/RAG/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from unstructured.chunking.title import chunk_by_title \n",
    "from unstructured.partition.pdf import partition_pdf \n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace\n",
    "from langchain_huggingface.llms import HuggingFaceEndpoint\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_core.messages import HumanMessage, SystemMessage \n",
    "from dotenv import load_dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcaaa35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ff8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_document(file_path: str):\n",
    "    \"\"\"Extract elements from PDF using unstructured.io\"\"\"\n",
    "    print(f\"Partitioning document: {file_path}\")\n",
    "    \n",
    "    elements = partition_pdf(\n",
    "        filename=file_path,  # Path to your PDF file\n",
    "        strategy=\"hi_res\", # Use the most accurate (but slower) processing method of extraction\n",
    "        infer_table_structure=True, # Keep tables as structured HTML, not jumbled text\n",
    "        extract_image_block_types=[\"Image\"], # Grab images found in the PDF\n",
    "        extract_image_block_to_payload=True # Store images as base64 data you can actually use\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracted {len(elements)} elements\")\n",
    "    return elements\n",
    "\n",
    "\n",
    "def create_chunks_by_title(elements):\n",
    "    \"\"\"Create intelligent chunks using title based strategy\"\"\"\n",
    "    print(\"Creating smart chunks.....\")\n",
    "    chunks = chunk_by_title(\n",
    "        elements=elements,\n",
    "        max_characters=3000, # Hard limit - never exceed 3000 characters per chunk\n",
    "        new_after_n_chars= 2400, # Try to start a new chunk after 2400 characters \n",
    "        combine_text_under_n_chars= 500, # merge tiny chunks under 500 chars with neighbours\n",
    "    )\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def separate_content_types(chunk):\n",
    "    # basically separate the individual elements by type\n",
    "    content_types = {\n",
    "        'text': chunk.text,\n",
    "        'images': [],\n",
    "        'tables': [],\n",
    "        'types' : ['text']\n",
    "    }\n",
    "\n",
    "    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "            # print(element.to_dict())\n",
    "            element_type = type(element).__name__\n",
    "\n",
    "            if element_type == \"Table\":\n",
    "                table_html = getattr(element.metadata, \"text_as_html\")\n",
    "                content_types['tables'].append(table_html)\n",
    "                content_types['types'].append('table')\n",
    "                \n",
    "            if element_type == \"Image\":\n",
    "                image_base64 = getattr(element.metadata, \"image_base64\")\n",
    "                content_types['images'].append(image_base64)\n",
    "                content_types['types'].append('image')\n",
    "            \n",
    "    content_types['types'] = list(set(content_types['types']))\n",
    "\n",
    "    return content_types\n",
    "\n",
    "def create_ai_enhanced_summary(text: str, images: List[str] | None, tables: List[str] | None):\n",
    "    print(\"Inside the Create AI Enhanced Summary function\")\n",
    "    \"\"\"Create an AI enhanced summary for multimodal content\"\"\"\n",
    "    try:\n",
    "\n",
    "\n",
    "        hf_endpoint = HuggingFaceEndpoint(\n",
    "            model=\"CohereLabs/aya-vision-32b\", # you have to make sure that this model has an InferenceProvider on the HuggingFace Website.\n",
    "            task=\"conversational\",\n",
    "            temperature=0,\n",
    "            provider=\"auto\"\n",
    "        )\n",
    "\n",
    "        llm = ChatHuggingFace(llm=hf_endpoint)\n",
    "\n",
    "        prompt_text = f\"\"\"You are creating a searchable description for document content retrieval.\n",
    "        CONTENT TO ANALYZE:\n",
    "        TEXT CONTENT:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "\n",
    "        if tables:\n",
    "            prompt_text += \"TABLES:\\n\"\n",
    "            for i, table in enumerate(tables):\n",
    "                prompt_text += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "            prompt_text += \"\"\"\n",
    "            YOUR TASK:\n",
    "            Generate a comprehensive, searchable description that covers:\n",
    "\n",
    "            1. Key facts, numbers, and data points from text and tables\n",
    "            2. Main topics and concepts discussed  \n",
    "            3. Questions this content could answer\n",
    "            4. Visual content analysis (charts, diagrams, patterns in images)\n",
    "            5. Alternative search terms users might use\n",
    "\n",
    "            Make it detailed and searchable - prioritize findability over brevity.\n",
    "\n",
    "            SEARCHABLE DESCRIPTION:\n",
    "            \"\"\"\n",
    "        \n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "\n",
    "        if images:\n",
    "            for image in images:\n",
    "                message_content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}\n",
    "                })\n",
    "\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"AI Summary Failed BECAUSE {e}\") \n",
    "        summary = f\"{text[:300]}...\"\n",
    "        return summary\n",
    "\n",
    "\n",
    "\n",
    "def summarise_chunks(chunks):\n",
    "    \"\"\"Process all chunks with AI summaries\"\"\"\n",
    "    print(f\"Processing chunks with AI summaries....\")\n",
    "\n",
    "    langchain_documents = []\n",
    "    total_chunks = len(chunks)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Procesing chunk {i+1} / {total_chunks}\")\n",
    "        content_data = separate_content_types(chunk)\n",
    "        if content_data['images'] or content_data['tables']:\n",
    "            print(f\"Creating AI summary for chunk {i+1}.\")\n",
    "            try:\n",
    "                enhanced_content = create_ai_enhanced_summary(content_data['text'], content_data['images'], content_data['tables'])\n",
    "            except Exception as e:\n",
    "                print(f\"AI Summary Failed : {e}\")\n",
    "                enhanced_content = content_data['text']\n",
    "        else:\n",
    "            print(f\"Using only raw text (no tables / images)\")\n",
    "            enhanced_content = content_data['text']\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=enhanced_content, \n",
    "            metadata = {\n",
    "                \"original_content\": json.dumps({\n",
    "                    \"raw_text\": content_data['text'],\n",
    "                    \"images\": content_data['images'],\n",
    "                    \"tables\": content_data['tables'],\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "\n",
    "        langchain_documents.append(doc)\n",
    "    \n",
    "    print(f\"Processed {len(langchain_documents)} chunks!\")\n",
    "    return langchain_documents\n",
    "        \n",
    "def create_vector_store(documents, persist_directory=\"db/multimodal\"):\n",
    "    \"\"\"Create and persist ChromaDB vector store\"\"\"\n",
    "    print(\"ðŸ”® Creating embeddings and storing in ChromaDB...\")\n",
    "        \n",
    "    embedding_model = HuggingFaceEmbeddings(model=\"intfloat/e5-large-v2\")\n",
    "    \n",
    "    # Create ChromaDB vector store\n",
    "    print(\"--- Creating vector store ---\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory, \n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    print(\"--- Finished creating vector store ---\")\n",
    "    \n",
    "    print(f\"âœ… Vector store created and saved to {persist_directory}\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4040a5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning document: ./docs/1706.03762v7.pdf\n",
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 215 elements\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./docs/1706.03762v7.pdf\" \n",
    "elements = partition_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1674c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating smart chunks.....\n",
      "Created 25 chunks\n"
     ]
    }
   ],
   "source": [
    "chunks = create_chunks_by_title(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "294d2845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunks with AI summaries....\n",
      "Procesing chunk 1 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 2 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 3 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 4 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 5 / 25\n",
      "Creating AI summary for chunk 5.\n",
      "Inside the Create AI Enhanced Summary function\n",
      "Procesing chunk 6 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 7 / 25\n",
      "Creating AI summary for chunk 7.\n",
      "Inside the Create AI Enhanced Summary function\n",
      "Procesing chunk 8 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 9 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 10 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 11 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 12 / 25\n",
      "Creating AI summary for chunk 12.\n",
      "Inside the Create AI Enhanced Summary function\n",
      "Procesing chunk 13 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 14 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 15 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 16 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 17 / 25\n",
      "Creating AI summary for chunk 17.\n",
      "Inside the Create AI Enhanced Summary function\n",
      "Procesing chunk 18 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 19 / 25\n",
      "Creating AI summary for chunk 19.\n",
      "Inside the Create AI Enhanced Summary function\n",
      "Procesing chunk 20 / 25\n",
      "Creating AI summary for chunk 20.\n",
      "Inside the Create AI Enhanced Summary function\n",
      "Procesing chunk 21 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 22 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 23 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 24 / 25\n",
      "Using only raw text (no tables / images)\n",
      "Procesing chunk 25 / 25\n",
      "Creating AI summary for chunk 25.\n",
      "Inside the Create AI Enhanced Summary function\n",
      "Processed 25 chunks!\n"
     ]
    }
   ],
   "source": [
    "processed_chunks = summarise_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b040335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”® Creating embeddings and storing in ChromaDB...\n",
      "--- Creating vector store ---\n",
      "--- Finished creating vector store ---\n",
      "âœ… Vector store created and saved to db/multimodal\n"
     ]
    }
   ],
   "source": [
    "db = create_vector_store(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37efdaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='b7466f2f-0821-4685-ac43-1f17649ccaf7', metadata={'original_content': '{\"raw_text\": \"4 Why Self-Attention\\\\n\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1,...,xn) to another sequence of equal length (z1,...,zn), with xi,zi \\\\u2208 Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\\\n\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\\\n\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\\\n\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n\\\\n6\\\\n\\\\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\\\\n\\\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\", \"images\": [], \"tables\": []}'}, page_content='4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1,...,xn) to another sequence of equal length (z1,...,zn), with xi,zi âˆˆ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\\n\\n6\\n\\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k Â· n Â· d + n Â· d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.'), Document(id='16333c9b-aebe-4459-991e-c5ef39b3539b', metadata={'original_content': '{\"raw_text\": \"2 Background\\\\n\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\\\n\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\n\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\\\n\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\", \"images\": [], \"tables\": []}'}, page_content='2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].'), Document(id='b42f9dec-f5af-4ea5-8f2b-4b8b9e6de4dd', metadata={'original_content': '{\"raw_text\": \"As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\\\n\\\\n5 Training\\\\n\\\\nThis section describes the training regime for our models.\\\\n\\\\n5.1 Training Data and Batching\\\\n\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\", \"images\": [], \"tables\": []}'}, page_content='As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n5 Training\\n\\nThis section describes the training regime for our models.\\n\\n5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.')]\n"
     ]
    }
   ],
   "source": [
    "query = \"According to table 1, what is the complexity per layer of self attention?\"\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "relevant_docs = retriever.invoke(query)\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fddacce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_answer(relevant_docs, query):\n",
    "    try:\n",
    "        hf_endpoint = HuggingFaceEndpoint(\n",
    "            model=\"CohereLabs/aya-vision-32b\", # you have to make sure that this model has an InferenceProvider on the HuggingFace Website.\n",
    "            task=\"conversational\",\n",
    "            temperature=0,\n",
    "            provider=\"auto\"\n",
    "        )\n",
    "\n",
    "        llm = ChatHuggingFace(llm=hf_endpoint)\n",
    "\n",
    "        prompt_text = f\"\"\"Based on the following documents, please answer this question : {query}.\n",
    "\n",
    "        CONTENT TO ANALYZE:\n",
    "        \"\"\"\n",
    "\n",
    "        for i, doc in enumerate(relevant_docs):\n",
    "            prompt_text += f\"----- DOCUMENT {i+1} -----\\n\"\n",
    "            if \"original_content\" in doc.metadata:\n",
    "                og_data = json.loads(doc.metadata[\"original_content\"])\n",
    "                \n",
    "                raw_text = og_data.get(\"raw_text\", \"\")\n",
    "                if raw_text:\n",
    "                    prompt_text += f\"TEXT : \\n{raw_text}\\n\\n\"\n",
    "                \n",
    "                raw_tables = og_data.get(\"tables\", [])\n",
    "                if raw_tables:\n",
    "                    prompt_text += \"TABLES: \\n\"\n",
    "                    for j, table in enumerate(raw_tables):\n",
    "                        prompt_text += f\"Table {j+1}:\\n{table}\\n\"\n",
    "                \n",
    "        prompt_text += \"\"\"\n",
    "        Please provide a clear, comprehensive answer using the text, tables, and images above. If the documents don't contain sufficient information to answer the question, say \"I don't have enough information to answer that question based on the provided documents.\"\n",
    "\n",
    "        ANSWER:\"\"\"\n",
    "\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "\n",
    "        images = og_data.get(\"images\", [])\n",
    "        if images:\n",
    "            for image in images:\n",
    "                message_content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}\n",
    "                })\n",
    "        \n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Problem answering the question : {e}\")\n",
    "        return \"Sorry, encountered a problem when answering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59b07ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer = generate_final_answer(relevant_docs, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e32ab92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensionality of the embeddings in the model described in the documents is dmodel = 512. This is evident from Document 2, which states that \"the dimensionality of input and output is dmodel = 512.\" This dimensionality is consistent across the input embeddings, output embeddings, and the positional encodings added to the input embeddings. The use of a consistent dimensionality allows for the efficient summation of the input embeddings and positional encodings, as well as the application of the feed-forward network and attention mechanisms within the model.\n"
     ]
    }
   ],
   "source": [
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
